# winequality
Consumers and manufacturers value wine's quality. Product quality certification boosts sales if wine manufacturers maintains the quality. Worldwide, wine is a popular beverage, and enterprises employ quality certification to improvise their market value. Previously, product quality testing was done at the end of production, which was time-consuming and expensive due to the necessity for human experts to analyse product quality. Every human has their own take on the test, so categorizing wine quality based on humans is difficult. There are various wine quality predictors, however not all are relevant. The project tries to determine what wine features are crucial to produce a positive result utilising supervised machine learning classification methods such as Logistic Regression, Na√Øve Bayes and some tree based and ensemble methods on a wine quality dataset. The aim also to use some unsupervised methods like K-means and Agglomerative clustering. The UCI machine learning repository has the wine dataset (Cortez et al., 2009). The collection includes red and white "Vinho Verde" wine variations. It contains machine learning datasets.
The supervised learning method performed better than unsupervised learning method. The dataset has several data points, which helped the model train and predict new values. The random forest model achieved 85% of test accuracy and it is believed that with hyper parameter tuning we can achieve better results. However, this was observed in the confusion matrix where false negative and false positive are nearly 0 for the classes which are extreme and have been balanced by SMOTE.
As an unsupervised model, K-means was weak. Even if grouping wine by quality was the primary purpose, there may be no logical connection between the characteristics that were selected. Because the model is sensitive to outliers, or because the data overlapping makes it difficult for the algorithm to discern that there are two groups, this could be the most likely explanation. The PCA was able to explain 86.5 percent of the variance in the 11 features down to just six. Additionally, the model's sillhoutte score improved to 0.27 after PCA was applied, but it was still below the threshold required to match the data points with their respective clusters. Even after pca, my inertia score remained high, indicating that I was still extracting more features than was necessary. As a result, K-means cannot be used as a model for this situation.
